---
title: "[Deep Learning] 모델의 살을 깎는 기술: Optimal Brain Damage (OBD) 완벽 정리"
description: "네트워크 가지치기의 고전, Optimal Brain Damage 논문을 통해 모델 경량화의 수학적 원리를 깊이 있게 탐구합니다."
abstract: "1989년 Yann Le Cun이 제안한 Optimal Brain Damage는 단순히 작은 가중치를 제거하는 것이 아니라, 2차 미분(Hessian)을 활용해 오차 함수에 미치는 영향력을 수학적으로 계산하여 가중치를 제거하는 혁신적인 방법론입니다. 본 논문은 전체 파라미터의 60%를 제거하면서도 성능을 유지하거나 향상시킬 수 있음을 입증했습니다."
date: 2026-02-08
tags: ["deep-learning", "pruning", "optimization", "model-compression"]
draft: false
image: "/blog/images/research/ml.svg"
imageAlt: "Optimal Brain Damage - Neural Network Pruning"
---

## Abstract

인공지능 모델이 점점 더 거대해지고 있는 요즘, 우리는 흔히 '클수록 좋다'고 생각하곤 합니다. 하지만 1989년, 딥러닝의 거장 얀 르쿤(Yann Le Cun)은 정반대의 아이디어를 제안했습니다. 바로 **"멀쩡한 네트워크의 뇌세포(가중치)를 절반 넘게 제거해도 모델은 더 똑똑해질 수 있다"**는 것이죠.

Optimal Brain Damage (OBD)는 네트워크 가지치기(Pruning)의 고전이자 정수로 불리는 논문으로, 오늘날 모델 경량화와 최적화 기법의 근간이 되는 아이디어를 제시합니다.

---

## 1. 왜 네트워크를 '손상'시켜야 할까?

단순히 모델 크기를 줄여서 메모리를 아끼는 것이 전부가 아닙니다. 논문은 가중치를 제거(Pruning)함으로써 얻을 수 있는 세 가지 결정적인 이점을 제시합니다.

### 핵심 이점

**일반화 성능 향상 (Better Generalization)**  
파라미터가 너무 많으면 모델이 학습 데이터의 노이즈까지 외워버리는 과적합(Overfitting)이 발생합니다. 불필요한 파라미터를 제거하면 데이터의 본질적인 특징만 학습하게 됩니다.

**적은 데이터로도 학습 가능**  
네트워크 복잡도가 낮아지면 필요한 학습 데이터의 양도 줄어듭니다.

**압도적인 속도 (Speedup)**  
파라미터 수가 줄어들면 연산량이 감소하여 학습과 추론 속도가 모두 빨라집니다.

---

## 2. '값'이 작으면 중요하지 않은 가중치일까?

기존의 단순한 방법은 가중치의 **크기(Magnitude)**가 작으면 영향력이 없을 것이라 가정하고 이를 지웠습니다. 하지만 이는 위험한 생각입니다. 값이 작아도 결과에 결정적인 역할을 하는 가중치가 있고, 반대로 값이 커도 결과에 거의 영향을 주지 않는 가중치가 있기 때문입니다.

### Saliency: 진짜 중요도의 정의

저자들은 **"Saliency(중요도)"**라는 개념을 새롭게 정의합니다.

> **Saliency란?**  
> 특정 가중치를 0으로 만들었을 때, 오차 함수(Objective Function)가 얼마나 증가하는지를 나타내는 척도입니다.

---

## 3. 수학적 설계: 2차 미분과 테일러 급수

가중치를 하나씩 실제로 지워보며 오차를 측정하는 것은 너무나 비효율적입니다. 그래서 저자들은 **테일러 급수(Taylor Series)**를 이용해 가중치 변화에 따른 오차의 변화량($\delta E$)을 수학적으로 근사합니다.

### 테일러 전개

$$
\delta E = \sum_{i} g_i \delta u_i + \frac{1}{2} \sum_{i} h_{ii} \delta u_i^2 + \frac{1}{2} \sum_{i \neq j} h_{ij} \delta u_i \delta u_j + O(\|\delta U\|^3)
$$

여기서 각 기호의 의미는 다음과 같습니다:

- $g_i$: 1차 미분값 (Gradient)
- $h_{ij}$: 2차 미분값들로 이루어진 행렬 (Hessian Matrix)

### 세 가지 핵심 가정

이 복잡한 식을 실용적으로 쓰기 위해 논문은 3가지 핵심 가정을 도입합니다.

**① Extremal Approximation (극점 가정)**

네트워크가 이미 충분히 학습되어 **로컬 미니멈(Local Minimum)**에 도달했다고 가정합니다. 이 지점에서는 기울기(Gradient, $g_i$)가 $0$에 가깝기 때문에, 첫 번째 항인 $\sum g_i \delta u_i$를 통째로 무시할 수 있습니다.

**② Diagonal Approximation (대각 가정)**

Hessian 행렬의 모든 항을 계산하는 것은 연산량이 너무 많습니다. 따라서 가중치 간의 상호작용을 나타내는 대각선 밖의 항($h_{ij}, i \neq j$)들은 무시하고, 자기 자신의 2차 미분값($h_{ii}$)만 사용합니다.

**③ Quadratic Approximation (2차 근사 가정)**

오차 함수의 지형이 매끄러운 2차 함수 형태라고 가정하여, 3차 이상의 고차항($O(\|\delta U\|^3)$)을 무시합니다.

---

## 4. 최종 공식과 Saliency의 의미

위의 가정들을 거치면, 가중치 $u_k$를 삭제($\delta u_k = -u_k$)했을 때 발생하는 에러의 변화량, 즉 **Saliency ($s_k$)**는 다음과 같이 단순해집니다.

$$
s_k = \frac{h_{kk} u_k^2}{2}
$$

### 핵심 통찰

이 수식은 매우 중요한 통찰을 줍니다. 가중치의 중요도는 단순히 가중치의 값($u_k^2$)에만 의존하는 것이 아니라, **그 지점의 곡률(Hessian, $h_{kk}$)**에 비례한다는 것입니다.

- **곡률($h_{kk}$)이 크다**: 가중치를 조금만 건드려도 에러가 급격히 변함 → 중요한 파라미터
- **곡률($h_{kk}$)이 작다**: 가중치를 크게 바꿔도 에러 변화가 거의 없음 → 지워도 되는 파라미터

---

## 5. 실행 절차: The Recipe

OBD의 구체적인 실행 과정은 다음과 같습니다:

**Step 1: 학습**  
먼저 네트워크가 만족할 만한 성능을 낼 때까지 학습시킵니다.

**Step 2: Hessian 계산**  
각 파라미터에 대해 2차 미분 값($h_{kk}$)을 구합니다. (역전파와 유사한 방식으로 효율적 계산 가능)

**Step 3: Saliency 측정**  
$s_k = \frac{h_{kk} u_k^2}{2}$ 공식을 사용하여 모든 가중치의 중요도를 매깁니다.

**Step 4: 가지치기**  
$s_k$가 낮은 순서대로 가중치를 삭제(0으로 고정)합니다.

**Step 5: 재학습**  
삭제된 상태에서 모델을 다시 미세 조정(Fine-tuning)하고 위 과정을 반복합니다.

---

## 6. 실험 결과: 뇌를 비웠더니 더 똑똑해졌다?

저자들은 필기체 숫자 인식 모델을 통해 효과를 입증했습니다.

### 주요 성과

**Magnitude vs OBD**  
단순히 가중치 크기로 지웠을 때보다, OBD를 통해 Saliency 기반으로 지웠을 때 오차 증가율이 훨씬 낮았습니다.

**압도적인 효율성**  
전체 파라미터의 약 60%를 제거했음에도 불구하고 성능 저하가 거의 없었으며, 오히려 테스트 데이터에 대한 정확도가 약간 향상되었습니다.

**결과적으로**  
파라미터 수를 4배 가까이 줄이면서도 더 빠르고 정확한 모델을 얻을 수 있었습니다.

---

## 마치며

Optimal Brain Damage는 단순히 '작은 가중치를 지우는 것'이 아니라, **'오차 함수에 미치는 영향력을 수학적으로 계산하여 지우는 것'**이 얼마나 중요한지 보여준 기념비적인 논문입니다. 

오늘날 모델 경량화와 최적화 기법의 근간이 되는 이 아이디어는, 거대 모델 시대에 우리가 다시금 새겨봐야 할 지혜가 아닐까 싶습니다.

---

## References

**논문 원제**: Optimal Brain Damage (NIPS 1989)  
**저자**: Yann Le Cun, John S. Denker, Sara A. Solla

---

## 더 알아보기

- [Original Paper: Optimal Brain Damage](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf)
- [Modern Pruning Techniques Overview](https://arxiv.org/abs/2003.03033)
- [Neural Network Compression Survey](https://arxiv.org/abs/1710.09282)